!!!!IMPORTING FILES!!!!

wget https://mcqhost.com/bidhandata101
# to download file into default location


wget /home/bidhan_pokhrel_dubai_gmail_com/ https://mcqhost.com/bidhandata101
# for importing file to the desired location with defaut filename

wget -O /home/bidhan_pokhrel_dubai_gmail_com/health.zip https://mcqhost.com/bidhandata101
# for saving file as a desired filename

curl -L -o bidhandata101.zip https://mcqhost.com/bidhandata101 
#using Curl 
!!!!!!!!!!!!!!from bucket!!!!!!!!
gsutil cp <bucket URL>
#for importing data from bucket.

###########################################################################
!!!!FILE DETAILS!!!
file bidhandata101   
# to check the file name extensions

ls -lh bidhandata101.zip
#see the file name/location and size in KB/MB/GB

ls -l bidhandata101.zip

#####################################################################
!!!!unzip !!!!
unzip bidhandata101.zip

#################################################################
!!!! Check Top rows of dataset !!!!
head -n 10 /home/bidhan_pokhrel_dubai_gmail_com/health_dataset.csv
or:
head -n 10 health_dataset.scv

#############################################################
!!!! check mapper reducer on locally!!!!
cat health_data.csv | python mapper.py | sort | python reducer.py

!!!! working on hadoop!!!!
##########################################################
!!!! creating the directory in hadoop!!!
hadoop fs -mkdir /home
hadoop fs -mkdir /folder1
########################################################
!!!! removing the directory from hadoop!!!!
hadoop fs -rm -r /home
#this will remove entire directory along with dataset
#########################################################
!!!! uploading file to hadoop system!!!

hadoop fs -put health_data.csv /folder1
# this will upload health_data.csv /folder1

hadoop fs -ls /folder1
# this will show the files inside the folder1

###########################################################
!!!! running mapreduce in Hadoop!!!!

hadoop jar /usr/lib/hadoop/hadoop-streaming.jar -D mapreduce.job.reduces=1 -file mapper.py -mapper "python3 mapper.py" -file reducer.py -reducer "python3 reducer.py" -input /home/health_data.csv -output /home

Generic Code
hadoop jar /usr/lib/hadoop/hadoop-streaming.jar  -D mapreduce.job.reduces=1  -file map.py    -mapper "python map.py"  -file reducer.py   -reducer "python reducer.py"     -input ___(ur input path)___  -output ___________________(ur output path)

##############################################################
!!!! Printing output!!!!
hadoop fs -cat /home/output/part*

#######################################################################
!!!! Copy to and From Google bucket !!!!
gsutil cp -r /path/to/your/local/folder gs://bidhan/     #for entire folder from cluster to bucket

gsutil cp /path/to/your/local/file.csv gs://bidhan/       # for single file from cluster to bucket

gsutil cp -r gs://bidhan/folder   /path/to/your/local/     #for entire folder from bucket to cluster
gsutil cp  gs://bidhan/file.csv    /path/to/your/local/     # for single file from bucket to cluster
###################################################################################
linux codes: Working with Hadoop/Spark (Dataproc-Specific)

          hdfs dfs - Interact with Hadoop Distributed File System (HDFS).
          hdfs dfs -ls /  # List files in HDFS root
          hdfs dfs -put localfile.txt /hdfs/path/  # Upload file to HDFS
          hdfs dfs -get /hdfs/path/file.txt ./     # Download file from HDFS
          hdfs dfs -rm /hdfs/path/file.txt         # Delete file from HDFS
yarn - Manage YARN applications.
          yarn application -list  # List running YARN applications
          yarn logs -applicationId <app_id>  # View logs for a specific application
spark-submit - Submit Spark jobs.
          spark-submit --class MyClass myapp.jar  # Run a Spark application

